{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83d\udea7 AU-Ggregates AI Server \u2014 Google Colab Deployment\n",
    "\n",
    "**Pipeline:** Mistral-7B-Instruct-v0.2 (4-bit) \u2192 T5-text-to-SQL \u2192 Supabase\n",
    "\n",
    "## Before you start:\n",
    "1. **Runtime \u2192 Change runtime type \u2192 T4 GPU** (required!)\n",
    "2. Get your **ngrok auth token** (free): https://dashboard.ngrok.com/get-started/your-authtoken\n",
    "3. Get your **HuggingFace token** (free): https://huggingface.co/settings/tokens\n",
    "4. Have your **Supabase URL + anon key** ready\n",
    "\n",
    "## Steps:\n",
    "Run each cell in order (Shift+Enter). The whole setup takes ~5 minutes.\n",
    "\n",
    "| Cell | What it does | Time |\n",
    "|------|-------------|------|\n",
    "| 1 | Install Python packages | ~2 min |\n",
    "| 2 | Set your secrets/keys | instant |\n",
    "| 3 | Clone your GitHub repo | ~10 sec |\n",
    "| 4 | Verify T4 GPU is active | instant |\n",
    "| 5 | Login to HuggingFace | instant |\n",
    "| 6 | Start server + ngrok tunnel | ~2 min (first load downloads Mistral ~4GB) |\n",
    "| 7 | Test the AI with a query | ~30 sec |\n",
    "| 8 | Health check | instant |"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 1: Install all dependencies\n",
    "# ============================================================\n",
    "# Core web framework\n",
    "!pip install -q fastapi uvicorn[standard] pydantic python-multipart\n",
    "\n",
    "# Database\n",
    "!pip install -q \"supabase>=2.0.0,<2.28.0\" psycopg2-binary\n",
    "\n",
    "# AI Models (Mistral 7B + T5 + Semantic Intent)\n",
    "!pip install -q torch transformers accelerate bitsandbytes sentencepiece\n",
    "!pip install -q sentence-transformers dateparser rapidfuzz\n",
    "\n",
    "# HuggingFace login\n",
    "!pip install -q huggingface_hub\n",
    "\n",
    "# Utilities\n",
    "!pip install -q python-dotenv requests loguru sentry-sdk python-json-logger\n",
    "!pip install -q sqlparse apscheduler\n",
    "\n",
    "# ngrok for public URL\n",
    "!pip install -q pyngrok\n",
    "\n",
    "# Jupyter async fix\n",
    "!pip install -q nest_asyncio\n",
    "\n",
    "print('\\n\\u2705 All dependencies installed!')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 2: Set your secrets\n",
    "# ============================================================\n",
    "# Option A: Use Colab Secrets (recommended - click the key icon in left sidebar)\n",
    "# Option B: Paste values directly below\n",
    "\n",
    "import os\n",
    "\n",
    "# --- Try Colab Secrets first, fall back to manual values ---\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    SUPABASE_URL    = userdata.get('SUPABASE_URL')\n",
    "    SUPABASE_KEY    = userdata.get('SUPABASE_KEY')\n",
    "    NGROK_TOKEN     = userdata.get('NGROK_TOKEN')\n",
    "    HF_TOKEN        = userdata.get('HF_TOKEN')\n",
    "    print('\\u2705 Loaded secrets from Colab Secrets')\n",
    "except Exception:\n",
    "    # --- Option B: Paste your values here ---\n",
    "    SUPABASE_URL    = 'YOUR_SUPABASE_URL'        # e.g. https://xxxxx.supabase.co\n",
    "    SUPABASE_KEY    = 'YOUR_SUPABASE_ANON_KEY'   # starts with eyJ...\n",
    "    NGROK_TOKEN     = 'YOUR_NGROK_AUTH_TOKEN'     # from ngrok dashboard\n",
    "    HF_TOKEN        = 'YOUR_HF_TOKEN'             # from huggingface.co/settings/tokens\n",
    "    print('\\u2705 Using manual secret values (make sure you filled them in!)')\n",
    "\n",
    "# --- Set environment variables for the app ---\n",
    "os.environ['SUPABASE_URL']         = SUPABASE_URL\n",
    "os.environ['SUPABASE_KEY']         = SUPABASE_KEY\n",
    "os.environ['MISTRAL_MODEL']        = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "os.environ['MISTRAL_QUANTIZATION'] = '4bit'\n",
    "os.environ['T5_MODEL_PATH']        = 'gaussalgo/T5-LM-Large-text2sql-spider'\n",
    "os.environ['ALLOWED_TABLES']       = 'ai_documents,Project,conversations'\n",
    "os.environ['API_PORT']             = '8000'\n",
    "os.environ['API_HOST']             = '0.0.0.0'\n",
    "os.environ['ENVIRONMENT']          = 'production'\n",
    "os.environ['CORS_ALLOW_ALL']       = 'true'  # Allow ngrok URLs\n",
    "\n",
    "# Validate\n",
    "assert SUPABASE_URL and not SUPABASE_URL.startswith('YOUR'), '\\u274c Fill in SUPABASE_URL!'\n",
    "assert SUPABASE_KEY and not SUPABASE_KEY.startswith('YOUR'), '\\u274c Fill in SUPABASE_KEY!'\n",
    "assert NGROK_TOKEN and not NGROK_TOKEN.startswith('YOUR'),   '\\u274c Fill in NGROK_TOKEN!'\n",
    "assert HF_TOKEN and not HF_TOKEN.startswith('YOUR'),         '\\u274c Fill in HF_TOKEN!'\n",
    "\n",
    "print(f'Supabase URL: {SUPABASE_URL[:40]}...')\n",
    "print('\\u2705 All secrets validated!')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 3: Clone your GitHub repo\n",
    "# ============================================================\n",
    "import os\n",
    "\n",
    "GITHUB_REPO = 'https://github.com/espinajc2004-max/auggregates-ai-data-look-up.git'\n",
    "\n",
    "REPO_DIR = '/content/auggregates-ai-data-look-up'\n",
    "\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    !git clone {GITHUB_REPO} {REPO_DIR}\n",
    "else:\n",
    "    !cd {REPO_DIR} && git pull\n",
    "    print(f'\\u2705 Updated existing repo at {REPO_DIR}')\n",
    "\n",
    "os.chdir(REPO_DIR)\n",
    "print(f'Working directory: {os.getcwd()}')\n",
    "\n",
    "# Verify key files exist\n",
    "for f in ['app/main.py', 'app/services/mistral_service.py', 'app/config/prompt_templates.py']:\n",
    "    assert os.path.exists(f), f'\\u274c Missing file: {f}'\n",
    "print('\\u2705 All key files present!')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 4: Verify GPU\n",
    "# ============================================================\n",
    "import torch\n",
    "\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "print(f'CUDA available:  {torch.cuda.is_available()}')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    vram_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f'GPU:             {gpu_name}')\n",
    "    print(f'VRAM:            {vram_gb:.1f} GB')\n",
    "    print('\\u2705 GPU ready! Mistral 4-bit needs ~5GB VRAM, T4 has 15GB. Good to go.')\n",
    "else:\n",
    "    print('\\u274c No GPU detected!')\n",
    "    print('Go to: Runtime \\u2192 Change runtime type \\u2192 T4 GPU')\n",
    "    print('Then re-run all cells from the top.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 5: Login to HuggingFace\n",
    "# ============================================================\n",
    "# Needed to download Mistral-7B-Instruct-v0.2 from HuggingFace\n",
    "# This model is NOT gated â€” no license agreement needed!\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token=HF_TOKEN)\n",
    "print('\\u2705 HuggingFace login successful!')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 6: Start FastAPI server + ngrok tunnel\n",
    "# ============================================================\n",
    "import subprocess\n",
    "import threading\n",
    "import time\n",
    "import requests as req\n",
    "from pyngrok import ngrok, conf\n",
    "\n",
    "# Make sure we're in the repo directory\n",
    "import os\n",
    "os.chdir('/content/auggregates-ai-data-look-up')\n",
    "\n",
    "# Configure ngrok\n",
    "conf.get_default().auth_token = NGROK_TOKEN\n",
    "\n",
    "# Start FastAPI in a background thread\n",
    "def run_server():\n",
    "    subprocess.run([\n",
    "        'python', '-m', 'uvicorn', 'app.main:app',\n",
    "        '--host', '0.0.0.0',\n",
    "        '--port', '8000',\n",
    "        '--workers', '1',\n",
    "        '--timeout-keep-alive', '120'\n",
    "    ])\n",
    "\n",
    "server_thread = threading.Thread(target=run_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "# Wait for server to be ready (with retry)\n",
    "print('\\u23f3 Starting FastAPI server...')\n",
    "print('   (First run downloads Mistral ~4GB + T5 ~770MB, may take a few minutes)')\n",
    "\n",
    "server_ready = False\n",
    "for i in range(60):  # Wait up to 5 minutes\n",
    "    try:\n",
    "        r = req.get('http://localhost:8000/api/health', timeout=3)\n",
    "        if r.status_code == 200:\n",
    "            server_ready = True\n",
    "            break\n",
    "    except:\n",
    "        pass\n",
    "    if i % 6 == 0 and i > 0:\n",
    "        print(f'   Still loading... ({i*5}s elapsed)')\n",
    "    time.sleep(5)\n",
    "\n",
    "if not server_ready:\n",
    "    print('\\u26a0\\ufe0f Server not responding yet, but starting ngrok anyway...')\n",
    "    print('   The server may still be loading models. Try Cell 8 (health check) in a minute.')\n",
    "else:\n",
    "    print('\\u2705 Server is running on port 8000')\n",
    "\n",
    "# Open ngrok tunnel\n",
    "tunnel = ngrok.connect(8000)\n",
    "public_url = tunnel.public_url\n",
    "\n",
    "print()\n",
    "print('=' * 60)\n",
    "print(f'\\ud83d\\ude80 SERVER IS LIVE!')\n",
    "print(f'\\ud83c\\udf10 Public URL:     {public_url}')\n",
    "print(f'\\ud83d\\udcac Chat endpoint:  {public_url}/api/chat/hybrid')\n",
    "print(f'\\ud83c\\udfe5 Health check:   {public_url}/api/health')\n",
    "print('=' * 60)\n",
    "print()\n",
    "print('\\u261d\\ufe0f Copy the Public URL above and use it in your frontend!')\n",
    "print('   Replace your localhost:8000 with this URL.')\n",
    "print()\n",
    "print('\\u26a0\\ufe0f  This URL changes every time you restart. For a stable URL,')\n",
    "print('   upgrade to ngrok paid plan or use a custom domain.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 7: Test the AI with a sample query\n",
    "# ============================================================\n",
    "import requests as req\n",
    "import json\n",
    "\n",
    "# Use public_url if ngrok worked, otherwise localhost\n",
    "try:\n",
    "    base_url = public_url\n",
    "except NameError:\n",
    "    base_url = 'http://localhost:8000'\n",
    "    print(f'\\u26a0\\ufe0f ngrok URL not available, using {base_url}\\n')\n",
    "\n",
    "# First check model loading status\n",
    "print('\\ud83d\\udd0d Checking model status...')\n",
    "try:\n",
    "    status = req.get(f'{base_url}/api/chat/hybrid/status', timeout=10).json()\n",
    "    print(f'   Mistral loaded: {status.get(\"mistral_loaded\")}')\n",
    "    print(f'   Loading in progress: {status.get(\"loading_in_progress\")}')\n",
    "    print(f'   Load attempts: {status.get(\"load_attempts\")}/{status.get(\"max_attempts\")}')\n",
    "    print(f'   Pipeline: {status.get(\"pipeline\")}\\n')\n",
    "except Exception as e:\n",
    "    print(f'   Could not check status: {e}\\n')\n",
    "\n",
    "print('\\ud83e\\uddea Sending test query: \"show me the francis gays expenses file\"')\n",
    "print('   (First query may be slow ~30-60s while models warm up)\\n')\n",
    "\n",
    "try:\n",
    "    response = req.post(\n",
    "        f'{base_url}/api/chat/hybrid',\n",
    "        json={'query': 'show me the francis gays expenses file'},\n",
    "        timeout=180  # 3 min timeout for first query\n",
    "    )\n",
    "\n",
    "    print(f'Status: {response.status_code}')\n",
    "    data = response.json()\n",
    "    print(f'Pipeline: {data.get(\"metadata\", {}).get(\"pipeline\", \"unknown\")}')\n",
    "    print(f'SQL Source: {data.get(\"metadata\", {}).get(\"sql_source\", \"n/a\")}')\n",
    "    print(f'Message: {data.get(\"message\", \"\")}')\n",
    "    print(f'Results: {data.get(\"metadata\", {}).get(\"row_count\", 0)} rows')\n",
    "    \n",
    "    if data.get('metadata', {}).get('pipeline') == 'mistral+t5':\n",
    "        sql_src = data.get('metadata', {}).get('sql_source', '')\n",
    "        print(f'\\n\\u2705 Full AI pipeline is working! (Mistral + T5, SQL from: {sql_src})')\n",
    "    elif data.get('metadata', {}).get('pipeline') == 'rule-based':\n",
    "        print('\\n\\u26a0\\ufe0f Using rule-based fallback (Mistral may still be loading)')\n",
    "        print('   Check model status above. Wait 1-2 minutes and try again.')\n",
    "    \n",
    "    print(f'\\nFull response:\\n{json.dumps(data, indent=2, ensure_ascii=False)}')\n",
    "\n",
    "except req.exceptions.Timeout:\n",
    "    print('\\u23f0 Request timed out. Models may still be loading.')\n",
    "    print('   Wait 2 minutes and re-run this cell.')\n",
    "except Exception as e:\n",
    "    print(f'\\u274c Error: {e}')\n",
    "    print('   Make sure Cell 6 completed successfully.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 8: Health check + Model status\n",
    "# ============================================================\n",
    "import requests as req\n",
    "\n",
    "# Use public_url if ngrok worked, otherwise localhost\n",
    "try:\n",
    "    base_url = public_url\n",
    "except NameError:\n",
    "    base_url = 'http://localhost:8000'\n",
    "\n",
    "try:\n",
    "    r = req.get(f'{base_url}/api/health', timeout=10)\n",
    "    print(f'Health: {r.status_code} {r.json()}')\n",
    "except Exception as e:\n",
    "    print(f'\\u274c Health check failed: {e}')\n",
    "\n",
    "try:\n",
    "    r = req.get(f'{base_url}/api/chat/hybrid/status', timeout=10)\n",
    "    status = r.json()\n",
    "    print(f'\\nModel Status:')\n",
    "    print(f'  Mistral loaded:     {status.get(\"mistral_loaded\")}')\n",
    "    print(f'  Loading in progress: {status.get(\"loading_in_progress\")}')\n",
    "    print(f'  Load attempts:      {status.get(\"load_attempts\")}/{status.get(\"max_attempts\")}')\n",
    "    print(f'  Active pipeline:    {status.get(\"pipeline\")}')\n",
    "    if status.get('mistral_loaded'):\n",
    "        print('\\n\\u2705 Mistral+T5 pipeline is ready!')\n",
    "    elif status.get('loading_in_progress'):\n",
    "        print('\\n\\u23f3 Models still loading... wait and re-run this cell.')\n",
    "    else:\n",
    "        print(f'\\n\\u26a0\\ufe0f Models not loaded after {status.get(\"load_attempts\")} attempts.')\n",
    "        print('   Check Colab logs for errors. Try: Runtime > Restart session, then re-run all cells.')\n",
    "except Exception as e:\n",
    "    print(f'\\u274c Model status check failed: {e}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 9: Debug - Check if Mistral loaded (run if pipeline=rule-based)\n",
    "# ============================================================\n",
    "# WARNING: Do NOT run this cell if Cell 6 (server) is running!\n",
    "#          Two Mistral instances will cause CUDA OOM on T4.\n",
    "#          Use this cell ONLY for standalone debugging (skip Cell 6).\n",
    "# ============================================================\n",
    "import sys, os, traceback\n",
    "os.chdir('/content/auggregates-ai-data-look-up')\n",
    "if '/content/auggregates-ai-data-look-up' not in sys.path:\n",
    "    sys.path.insert(0, '/content/auggregates-ai-data-look-up')\n",
    "\n",
    "# Fix Jupyter async issue (Colab already has an event loop running)\n",
    "!pip install -q nest_asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "print('Attempting to load Mistral+T5 directly...')\n",
    "print('(This will take 3-5 min on first run as it downloads ~4GB)\\n')\n",
    "\n",
    "try:\n",
    "    from app.services.mistral_service import MistralService\n",
    "    svc = MistralService()\n",
    "    print(f'Config: model={svc.config.model_name}')\n",
    "    print(f'Config: quant={svc.config.quantization}')\n",
    "    print(f'Config: device={svc.config.device}')\n",
    "    print()\n",
    "    svc._load_model()\n",
    "    print('\\n--- Model loaded! Testing inference ---')\n",
    "    import asyncio\n",
    "    result = asyncio.get_event_loop().run_until_complete(\n",
    "        svc.process_query('show all expenses', 'test-user')\n",
    "    )\n",
    "    print(f'Pipeline response: {result.get(\"response\", \"\")[:200]}')\n",
    "    print(f'SQL: {result.get(\"sql\", \"\")}')\n",
    "    print(f'Rows: {result.get(\"row_count\", 0)}')\n",
    "    print('\\nDone! Mistral+T5 is working.')\n",
    "except Exception as e:\n",
    "    print(f'Error: {type(e).__name__}: {e}')\n",
    "    traceback.print_exc()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## \ud83d\udcdd Notes\n",
    "\n",
    "**Keeping the server alive:**\n",
    "- Colab disconnects after ~90 min of inactivity (free tier)\n",
    "- Keep this tab open and active to prevent disconnection\n",
    "- If disconnected, just re-run all cells from Cell 1\n",
    "\n",
    "**Connecting your frontend:**\n",
    "- Use the ngrok Public URL from Cell 6 as your API base URL\n",
    "- Example: `fetch('https://xxxx.ngrok-free.app/api/chat/hybrid', { method: 'POST', ... })`\n",
    "- The URL changes every restart \u2014 update your frontend config each time\n",
    "\n",
    "**Performance:**\n",
    "- First query after startup: ~30-60 seconds (model warmup)\n",
    "- Subsequent queries: ~5-15 seconds\n",
    "- T4 GPU has 15GB VRAM \u2014 Mistral 4-bit uses ~5GB, plenty of headroom\n",
    "\n",
    "**Troubleshooting:**\n",
    "- `CUDA out of memory` \u2192 Runtime \u2192 Restart runtime, then re-run all cells\n",
    "- `ngrok error` \u2192 Check your auth token at https://dashboard.ngrok.com\n",
    "- `rule-based fallback` \u2192 Models still loading, wait 1-2 min and retry"
   ]
  }
 ]
}