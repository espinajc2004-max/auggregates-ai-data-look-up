{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83d\udea7 AU-Ggregates AI Server \u2014 Google Colab Deployment\n",
    "\n",
    "**Pipeline:** Mistral-7B-Instruct-v0.2 (4-bit) \u2192 T5-text-to-SQL \u2192 Supabase\n",
    "\n",
    "## Before you start:\n",
    "1. **Runtime \u2192 Change runtime type \u2192 T4 GPU** (required!)\n",
    "2. Get your **ngrok auth token** (free): https://dashboard.ngrok.com/get-started/your-authtoken\n",
    "3. Get your **HuggingFace token** (free): https://huggingface.co/settings/tokens\n",
    "4. Have your **Supabase URL + anon key** ready\n",
    "\n",
    "## Steps:\n",
    "Run each cell in order (Shift+Enter). The whole setup takes ~5 minutes.\n",
    "\n",
    "| Cell | What it does | Time |\n",
    "|------|-------------|------|\n",
    "| 1 | Install Python packages | ~2 min |\n",
    "| 2 | Set your secrets/keys | instant |\n",
    "| 3 | Clone your GitHub repo | ~10 sec |\n",
    "| 4 | Verify T4 GPU is active | instant |\n",
    "| 5 | Login to HuggingFace | instant |\n",
    "| 6 | Start server + ngrok tunnel | ~2 min (first load downloads Mistral ~4GB) |\n",
    "| 7 | Test the AI with a query | ~30 sec |\n",
    "| 8 | Health check | instant |"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 1: Install all dependencies\n",
    "# ============================================================\n",
    "# Core web framework\n",
    "!pip install -q fastapi uvicorn[standard] pydantic python-multipart\n",
    "\n",
    "# Database\n",
    "!pip install -q \"supabase>=2.0.0,<2.28.0\" psycopg2-binary\n",
    "\n",
    "# AI Models (Mistral 7B + T5)\n",
    "!pip install -q torch transformers accelerate bitsandbytes sentencepiece\n",
    "\n",
    "# HuggingFace login\n",
    "!pip install -q huggingface_hub\n",
    "\n",
    "# Utilities\n",
    "!pip install -q python-dotenv requests loguru sentry-sdk python-json-logger\n",
    "!pip install -q sqlparse apscheduler\n",
    "\n",
    "# ngrok for public URL\n",
    "!pip install -q pyngrok\n",
    "\n",
    "print('\\n\\u2705 All dependencies installed!')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 2: Set your secrets\n",
    "# ============================================================\n",
    "# Option A: Use Colab Secrets (recommended - click the key icon in left sidebar)\n",
    "# Option B: Paste values directly below\n",
    "\n",
    "import os\n",
    "\n",
    "# --- Try Colab Secrets first, fall back to manual values ---\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    SUPABASE_URL    = userdata.get('SUPABASE_URL')\n",
    "    SUPABASE_KEY    = userdata.get('SUPABASE_KEY')\n",
    "    NGROK_TOKEN     = userdata.get('NGROK_TOKEN')\n",
    "    HF_TOKEN        = userdata.get('HF_TOKEN')\n",
    "    GITHUB_TOKEN    = userdata.get('GITHUB_TOKEN')\n",
    "    print('\\u2705 Loaded secrets from Colab Secrets')\n",
    "except Exception:\n",
    "    # --- Option B: Paste your values here ---\n",
    "    SUPABASE_URL    = 'https://osbgvhkrpnpptaduhwdy.supabase.co'\n",
    "    SUPABASE_KEY    = 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6Im9zYmd2aGtycG5wcHRhZHVod2R5Iiwicm9sZSI6ImFub24iLCJpYXQiOjE3NjQ0ODc0NDMsImV4cCI6MjA4MDA2MzQ0M30.a3kXnOpaZi8a7uw_X5gkrCylMie3-Io6dG0VCGKsAKw'\n",
    "    NGROK_TOKEN     = '3A7bwToSPflS2SZnDP02DJvan8r_2z85mYrYWQeKDVNXne1mz'\n",
    "    HF_TOKEN        = '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'\n",
    "    GITHUB_TOKEN    = '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'\n",
    "    print('\\u2705 Using manual secret values (make sure you filled them in!)')\n",
    "\n",
    "# --- Set environment variables for the app ---\n",
    "os.environ['SUPABASE_URL']         = SUPABASE_URL\n",
    "os.environ['SUPABASE_KEY']         = SUPABASE_KEY\n",
    "os.environ['MISTRAL_MODEL']        = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "os.environ['MISTRAL_QUANTIZATION'] = '4bit'\n",
    "os.environ['T5_MODEL_PATH']        = 'cssupport/t5-small-awesome-text-to-sql'\n",
    "os.environ['ALLOWED_TABLES']       = 'ai_documents,Project,conversations'\n",
    "os.environ['API_PORT']             = '8000'\n",
    "os.environ['API_HOST']             = '0.0.0.0'\n",
    "os.environ['ENVIRONMENT']          = 'production'\n",
    "os.environ['CORS_ALLOW_ALL']       = 'true'  # Allow ngrok URLs\n",
    "\n",
    "# Validate\n",
    "assert SUPABASE_URL and not SUPABASE_URL.startswith('YOUR'), '\\u274c Fill in SUPABASE_URL!'\n",
    "assert SUPABASE_KEY and not SUPABASE_KEY.startswith('YOUR'), '\\u274c Fill in SUPABASE_KEY!'\n",
    "assert NGROK_TOKEN and not NGROK_TOKEN.startswith('YOUR'),   '\\u274c Fill in NGROK_TOKEN!'\n",
    "assert HF_TOKEN and not HF_TOKEN.startswith('YOUR'),         '\\u274c Fill in HF_TOKEN!'\n",
    "assert GITHUB_TOKEN and not GITHUB_TOKEN.startswith('YOUR'), '\\u274c Fill in GITHUB_TOKEN!'\n",
    "\n",
    "print(f'Supabase URL: {SUPABASE_URL[:40]}...')\n",
    "print('\\u2705 All secrets validated!')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 3: Clone your GitHub repo\n",
    "# ============================================================\n",
    "import os\n",
    "\n",
    "# Build authenticated clone URL (private repo needs token)\n",
    "GITHUB_REPO = f'https://{GITHUB_TOKEN}@github.com/espinajc2004-max/auggregates-ai-data-look-up.git'\n",
    "\n",
    "REPO_DIR = '/content/auggregates-ai-data-look-up'\n",
    "\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    !git clone {GITHUB_REPO} {REPO_DIR}\n",
    "else:\n",
    "    !cd {REPO_DIR} && git pull\n",
    "    print(f'\\u2705 Updated existing repo at {REPO_DIR}')\n",
    "\n",
    "os.chdir(REPO_DIR)\n",
    "print(f'Working directory: {os.getcwd()}')\n",
    "\n",
    "# Verify key files exist\n",
    "for f in ['app/main.py', 'app/services/mistral_service.py', 'app/config/prompt_templates.py']:\n",
    "    assert os.path.exists(f), f'\\u274c Missing file: {f}'\n",
    "print('\\u2705 All key files present!')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 4: Verify GPU\n",
    "# ============================================================\n",
    "import torch\n",
    "\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "print(f'CUDA available:  {torch.cuda.is_available()}')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    vram_gb = torch.cuda.get_device_properties(0).total_mem / 1024**3\n",
    "    print(f'GPU:             {gpu_name}')\n",
    "    print(f'VRAM:            {vram_gb:.1f} GB')\n",
    "    print('\\u2705 GPU ready! Mistral 4-bit needs ~5GB VRAM, T4 has 15GB. Good to go.')\n",
    "else:\n",
    "    print('\\u274c No GPU detected!')\n",
    "    print('Go to: Runtime \\u2192 Change runtime type \\u2192 T4 GPU')\n",
    "    print('Then re-run all cells from the top.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 5: Login to HuggingFace\n",
    "# ============================================================\n",
    "# Needed to download Mistral-7B-Instruct-v0.2 from HuggingFace\n",
    "# This model is NOT gated â€” no license agreement needed!\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token=HF_TOKEN)\n",
    "print('\\u2705 HuggingFace login successful!')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 6: Start FastAPI server + ngrok tunnel\n",
    "# ============================================================\n",
    "import subprocess\n",
    "import threading\n",
    "import time\n",
    "import requests as req\n",
    "from pyngrok import ngrok, conf\n",
    "\n",
    "# Make sure we're in the repo directory\n",
    "import os\n",
    "os.chdir('/content/auggregates-ai-data-look-up')\n",
    "\n",
    "# Configure ngrok\n",
    "conf.get_default().auth_token = NGROK_TOKEN\n",
    "\n",
    "# Start FastAPI in a background thread\n",
    "def run_server():\n",
    "    subprocess.run([\n",
    "        'python', '-m', 'uvicorn', 'app.main:app',\n",
    "        '--host', '0.0.0.0',\n",
    "        '--port', '8000',\n",
    "        '--workers', '1',\n",
    "        '--timeout-keep-alive', '120'\n",
    "    ])\n",
    "\n",
    "server_thread = threading.Thread(target=run_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "# Wait for server to be ready (with retry)\n",
    "print('\\u23f3 Starting FastAPI server...')\n",
    "print('   (First run downloads Mistral ~4GB + T5 ~250MB, may take a few minutes)')\n",
    "\n",
    "server_ready = False\n",
    "for i in range(60):  # Wait up to 5 minutes\n",
    "    try:\n",
    "        r = req.get('http://localhost:8000/api/health', timeout=3)\n",
    "        if r.status_code == 200:\n",
    "            server_ready = True\n",
    "            break\n",
    "    except:\n",
    "        pass\n",
    "    if i % 6 == 0 and i > 0:\n",
    "        print(f'   Still loading... ({i*5}s elapsed)')\n",
    "    time.sleep(5)\n",
    "\n",
    "if not server_ready:\n",
    "    print('\\u26a0\\ufe0f Server not responding yet, but starting ngrok anyway...')\n",
    "    print('   The server may still be loading models. Try Cell 8 (health check) in a minute.')\n",
    "else:\n",
    "    print('\\u2705 Server is running on port 8000')\n",
    "\n",
    "# Open ngrok tunnel\n",
    "tunnel = ngrok.connect(8000)\n",
    "public_url = tunnel.public_url\n",
    "\n",
    "print()\n",
    "print('=' * 60)\n",
    "print(f'\\ud83d\\ude80 SERVER IS LIVE!')\n",
    "print(f'\\ud83c\\udf10 Public URL:     {public_url}')\n",
    "print(f'\\ud83d\\udcac Chat endpoint:  {public_url}/api/chat/hybrid')\n",
    "print(f'\\ud83c\\udfe5 Health check:   {public_url}/api/health')\n",
    "print('=' * 60)\n",
    "print()\n",
    "print('\\u261d\\ufe0f Copy the Public URL above and use it in your frontend!')\n",
    "print('   Replace your localhost:8000 with this URL.')\n",
    "print()\n",
    "print('\\u26a0\\ufe0f  This URL changes every time you restart. For a stable URL,')\n",
    "print('   upgrade to ngrok paid plan or use a custom domain.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 7: Test the AI with a sample query\n",
    "# ============================================================\n",
    "import requests as req\n",
    "import json\n",
    "\n",
    "print('\\ud83e\\uddea Sending test query: \"pakita lahat ng expenses\"')\n",
    "print('   (First query may be slow ~30-60s while models warm up)\\n')\n",
    "\n",
    "try:\n",
    "    response = req.post(\n",
    "        f'{public_url}/api/chat/hybrid',\n",
    "        json={'query': 'pakita lahat ng expenses'},\n",
    "        timeout=180  # 3 min timeout for first query\n",
    "    )\n",
    "\n",
    "    print(f'Status: {response.status_code}')\n",
    "    data = response.json()\n",
    "    print(f'Pipeline: {data.get(\"metadata\", {}).get(\"pipeline\", \"unknown\")}')\n",
    "    print(f'Message: {data.get(\"message\", \"\")}')\n",
    "    print(f'Results: {data.get(\"metadata\", {}).get(\"row_count\", 0)} rows')\n",
    "    \n",
    "    if data.get('metadata', {}).get('pipeline') == 'mistral+t5':\n",
    "        print('\\n\\u2705 Full AI pipeline is working! (Mistral + T5)')\n",
    "    elif data.get('metadata', {}).get('pipeline') == 'rule-based':\n",
    "        print('\\n\\u26a0\\ufe0f Using rule-based fallback (Mistral may still be loading)')\n",
    "        print('   Wait 1-2 minutes and try again.')\n",
    "    \n",
    "    print(f'\\nFull response:\\n{json.dumps(data, indent=2, ensure_ascii=False)}')\n",
    "\n",
    "except req.exceptions.Timeout:\n",
    "    print('\\u23f0 Request timed out. Models may still be loading.')\n",
    "    print('   Wait 2 minutes and re-run this cell.')\n",
    "except Exception as e:\n",
    "    print(f'\\u274c Error: {e}')\n",
    "    print('   Make sure Cell 6 completed successfully.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 8: Health check\n",
    "# ============================================================\n",
    "import requests as req\n",
    "\n",
    "try:\n",
    "    r = req.get(f'{public_url}/api/health', timeout=10)\n",
    "    print(f'Status: {r.status_code}')\n",
    "    print(r.json())\n",
    "    print('\\n\\u2705 Server is healthy!')\n",
    "except Exception as e:\n",
    "    print(f'\\u274c Health check failed: {e}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## \ud83d\udcdd Notes\n",
    "\n",
    "**Keeping the server alive:**\n",
    "- Colab disconnects after ~90 min of inactivity (free tier)\n",
    "- Keep this tab open and active to prevent disconnection\n",
    "- If disconnected, just re-run all cells from Cell 1\n",
    "\n",
    "**Connecting your frontend:**\n",
    "- Use the ngrok Public URL from Cell 6 as your API base URL\n",
    "- Example: `fetch('https://xxxx.ngrok-free.app/api/chat/hybrid', { method: 'POST', ... })`\n",
    "- The URL changes every restart \u2014 update your frontend config each time\n",
    "\n",
    "**Performance:**\n",
    "- First query after startup: ~30-60 seconds (model warmup)\n",
    "- Subsequent queries: ~5-15 seconds\n",
    "- T4 GPU has 15GB VRAM \u2014 Mistral 4-bit uses ~5GB, plenty of headroom\n",
    "\n",
    "**Troubleshooting:**\n",
    "- `CUDA out of memory` \u2192 Runtime \u2192 Restart runtime, then re-run all cells\n",
    "- `ngrok error` \u2192 Check your auth token at https://dashboard.ngrok.com\n",
    "- `rule-based fallback` \u2192 Models still loading, wait 1-2 min and retry"
   ]
  }
 ]
}