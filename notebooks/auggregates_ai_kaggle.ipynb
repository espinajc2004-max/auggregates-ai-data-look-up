{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {"name": "python3", "display_name": "Python 3"},
  "language_info": {"name": "python"},
  "kaggle": {"accelerator": "gpu", "isGpuEnabled": true, "isInternetEnabled": true}
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AU-Ggregates AI Server â€” Kaggle Deployment\n",
    "\n",
    "**Pipeline:** Phi-3-mini-4k-instruct (4-bit) > T5-text-to-SQL > Supabase\n",
    "\n",
    "## Before you start\n",
    "1. **Settings** (right panel) > **Accelerator** > **GPU T4 x2**\n",
    "2. **Settings** > **Internet** > make sure it's **ON**\n",
    "3. **Add-ons** > **Secrets** (or click the ðŸ”‘ key icon in right panel) > add these 4 secrets:\n",
    "   - `SUPABASE_URL` â€” your Supabase project URL (e.g. `https://xxx.supabase.co`)\n",
    "   - `SUPABASE_KEY` â€” your Supabase anon key (starts with `eyJ...`)\n",
    "   - `HF_TOKEN` â€” HuggingFace token (starts with `hf_...`)\n",
    "   - `NGROK_TOKEN` â€” ngrok auth token\n",
    "4. **IMPORTANT**: Toggle each secret's switch to **Attached** (ON) for this notebook\n",
    "5. After adding secrets, do **Run > Restart & Clear All Outputs** then re-run\n",
    "\n",
    "## Pipeline\n",
    "| Cell | What it does | Time |\n",
    "|------|-------------|------|\n",
    "| 1 | Install dependencies | ~2 min |\n",
    "| 2 | Set secrets/keys | instant |\n",
    "| 3 | Clone GitHub repo | ~10 sec |\n",
    "| 4 | Verify GPU | instant |\n",
    "| 5 | Login to HuggingFace | instant |\n",
    "| 6 | Start server + ngrok tunnel | ~2 min |\n",
    "| 6B | Monitor download progress | live |\n",
    "| 7 | Test AI query | ~30 sec |\n",
    "| 7B | Test query gating | ~30 sec |\n",
    "| 8 | Health check + debug | instant |"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 1: Install all dependencies\n",
    "# ============================================================\n",
    "# CRITICAL: Pin to single GPU before any torch import\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "# Core web framework\n",
    "!pip install -q fastapi uvicorn[standard] pydantic python-multipart\n",
    "\n",
    "# Database\n",
    "!pip install -q \"supabase>=2.0.0,<2.28.0\" psycopg2-binary\n",
    "\n",
    "# AI Models (Phi-3 + T5)\n",
    "!pip install -q \"transformers>=4.43.0,<4.48.0\" \"accelerate>=0.33.0\" \"bitsandbytes>=0.43.0\" sentencepiece\n",
    "!pip install -q sentence-transformers dateparser rapidfuzz\n",
    "\n",
    "# HuggingFace login\n",
    "!pip install -q huggingface_hub\n",
    "\n",
    "# Utilities\n",
    "!pip install -q python-dotenv requests loguru sentry-sdk python-json-logger\n",
    "!pip install -q sqlparse apscheduler\n",
    "\n",
    "# ngrok for public URL\n",
    "!pip install -q pyngrok\n",
    "\n",
    "# Jupyter async fix\n",
    "!pip install -q nest_asyncio\n",
    "\n",
    "print('\\nAll dependencies installed!')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 2: Set your secrets\n",
    "# ============================================================\n",
    "# HOW TO ADD SECRETS IN KAGGLE:\n",
    "#   1. Click the key icon (Add-ons) in the right panel\n",
    "#   2. Or go to: Settings > Add-ons > Secrets\n",
    "#   3. Add these 4 secrets (Label = value below, Value = your actual key):\n",
    "#      - SUPABASE_URL\n",
    "#      - SUPABASE_KEY\n",
    "#      - NGROK_TOKEN\n",
    "#      - HF_TOKEN\n",
    "#   4. IMPORTANT: Toggle each secret to \"Attached to notebook\" (switch ON)\n",
    "#   5. After adding secrets, you may need to RESTART the notebook session\n",
    "\n",
    "import os\n",
    "\n",
    "SUPABASE_URL = None\n",
    "SUPABASE_KEY = None\n",
    "NGROK_TOKEN  = None\n",
    "HF_TOKEN     = None\n",
    "\n",
    "# --- Method 1: Kaggle Secrets (recommended) ---\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    secrets = UserSecretsClient()\n",
    "    SUPABASE_URL = secrets.get_secret('SUPABASE_URL')\n",
    "    SUPABASE_KEY = secrets.get_secret('SUPABASE_KEY')\n",
    "    NGROK_TOKEN  = secrets.get_secret('NGROK_TOKEN')\n",
    "    HF_TOKEN     = secrets.get_secret('HF_TOKEN')\n",
    "    print('Loaded secrets from Kaggle Secrets')\n",
    "except ImportError:\n",
    "    print('WARNING: kaggle_secrets not available (not running on Kaggle?)')\n",
    "    print('         Will try environment variables or manual values below.')\n",
    "except Exception as e:\n",
    "    print(f'WARNING: Kaggle Secrets failed: {type(e).__name__}: {e}')\n",
    "    print('         Make sure you added ALL 4 secrets AND toggled them ON.')\n",
    "    print('         Go to: right panel > key icon > Add-ons > Secrets')\n",
    "\n",
    "# --- Method 2: Environment variables (e.g., from .env or Kaggle env) ---\n",
    "if not SUPABASE_URL:\n",
    "    SUPABASE_URL = os.environ.get('SUPABASE_URL', '')\n",
    "if not SUPABASE_KEY:\n",
    "    SUPABASE_KEY = os.environ.get('SUPABASE_KEY', '')\n",
    "if not NGROK_TOKEN:\n",
    "    NGROK_TOKEN = os.environ.get('NGROK_TOKEN', '')\n",
    "if not HF_TOKEN:\n",
    "    HF_TOKEN = os.environ.get('HF_TOKEN', '')\n",
    "\n",
    "# --- Method 3: Manual fallback (paste your values here if nothing else works) ---\n",
    "# Uncomment and fill in ONLY if Kaggle Secrets and env vars don't work:\n",
    "# SUPABASE_URL = 'https://xxxxx.supabase.co'\n",
    "# SUPABASE_KEY = 'eyJhbGciOi...'\n",
    "# NGROK_TOKEN  = '2xxx...'\n",
    "# HF_TOKEN     = 'hf_xxx...'\n",
    "\n",
    "# --- Set environment variables for the app ---\n",
    "os.environ['SUPABASE_URL']      = SUPABASE_URL or ''\n",
    "os.environ['SUPABASE_KEY']      = SUPABASE_KEY or ''\n",
    "os.environ['PHI3_MODEL']        = 'microsoft/Phi-3-mini-4k-instruct'\n",
    "os.environ['PHI3_QUANTIZATION'] = '4bit'\n",
    "os.environ['T5_MODEL_PATH']     = 'espinajc/t5-auggregates-text2sql'\n",
    "os.environ['ALLOWED_TABLES']    = 'ai_documents,Project,conversations'\n",
    "os.environ['API_PORT']          = '8000'\n",
    "os.environ['API_HOST']          = '0.0.0.0'\n",
    "os.environ['ENVIRONMENT']       = 'production'\n",
    "os.environ['CORS_ALLOW_ALL']    = 'true'\n",
    "\n",
    "# --- Validate with helpful error messages ---\n",
    "missing = []\n",
    "if not SUPABASE_URL or SUPABASE_URL.startswith('YOUR') or SUPABASE_URL.startswith('https://xxxxx'):\n",
    "    missing.append('SUPABASE_URL')\n",
    "if not SUPABASE_KEY or SUPABASE_KEY.startswith('YOUR') or SUPABASE_KEY.startswith('eyJhbGciOi...'):\n",
    "    missing.append('SUPABASE_KEY')\n",
    "if not NGROK_TOKEN or NGROK_TOKEN.startswith('YOUR') or NGROK_TOKEN.startswith('2xxx'):\n",
    "    missing.append('NGROK_TOKEN')\n",
    "if not HF_TOKEN or HF_TOKEN.startswith('YOUR') or HF_TOKEN.startswith('hf_xxx'):\n",
    "    missing.append('HF_TOKEN')\n",
    "\n",
    "if missing:\n",
    "    print()\n",
    "    print('=' * 60)\n",
    "    print(f'  MISSING SECRETS: {\", \".join(missing)}')\n",
    "    print('=' * 60)\n",
    "    print()\n",
    "    print('  To fix this in Kaggle:')\n",
    "    print('  1. Click the KEY icon in the right panel (Add-ons > Secrets)')\n",
    "    print('  2. Click \"Add a new secret\" for each missing one above')\n",
    "    print('  3. Label = secret name exactly as shown (e.g. SUPABASE_URL)')\n",
    "    print('  4. Value = your actual key/URL')\n",
    "    print('  5. Toggle the switch to ATTACH it to this notebook')\n",
    "    print('  6. After adding all secrets, RESTART the session:')\n",
    "    print('     Run > Restart & Clear All Outputs > then re-run cells')\n",
    "    print()\n",
    "    print('  Or uncomment the manual values in Method 3 above and paste your keys.')\n",
    "    raise ValueError(f'Missing secrets: {\", \".join(missing)}. See instructions above.')\n",
    "\n",
    "print(f'Supabase URL: {SUPABASE_URL[:40]}...')\n",
    "print('All 4 secrets validated!')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 3: Clone your GitHub repo\n",
    "# ============================================================\n",
    "import os\n",
    "\n",
    "GITHUB_REPO = 'https://github.com/espinajc2004-max/auggregates-ai-data-look-up.git'\n",
    "REPO_DIR = '/kaggle/working/auggregates-ai-data-look-up'\n",
    "\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    !git clone {GITHUB_REPO} {REPO_DIR}\n",
    "else:\n",
    "    !cd {REPO_DIR} && git pull\n",
    "    print(f'Updated existing repo at {REPO_DIR}')\n",
    "\n",
    "os.chdir(REPO_DIR)\n",
    "print(f'Working directory: {os.getcwd()}')\n",
    "\n",
    "# Verify key files exist\n",
    "for f in ['app/main.py', 'app/services/phi3_service.py', 'app/config/prompt_templates.py']:\n",
    "    assert os.path.exists(f), f'Missing file: {f}'\n",
    "print('All key files present!')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 4: Verify GPU\n",
    "# ============================================================\n",
    "import torch\n",
    "\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'CUDA:    {torch.cuda.is_available()}')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    print(f'GPUs visible: {n_gpus} (pinned to GPU 0)')\n",
    "    name = torch.cuda.get_device_name(0)\n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    vram = props.total_memory / 1024**3\n",
    "    print(f'GPU 0: {name} ({vram:.1f} GB VRAM)')\n",
    "    print('\\nGPU ready! Phi-3 4-bit needs ~2GB VRAM.')\n",
    "else:\n",
    "    print('No GPU! Go to Settings > Accelerator > GPU T4 x2')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 5: Login to HuggingFace\n",
    "# ============================================================\n",
    "from huggingface_hub import login\n",
    "login(token=HF_TOKEN)\n",
    "print('HuggingFace login successful!')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 6: Start FastAPI server + ngrok tunnel (IN-PROCESS)\n",
    "# ============================================================\n",
    "# Runs uvicorn INSIDE the notebook process to save ~3GB RAM.\n",
    "# ============================================================\n",
    "import threading, time, sys, os\n",
    "import requests as req\n",
    "import nest_asyncio\n",
    "import uvicorn\n",
    "from pyngrok import ngrok, conf\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "REPO_DIR = '/kaggle/working/auggregates-ai-data-look-up'\n",
    "os.chdir(REPO_DIR)\n",
    "if REPO_DIR not in sys.path:\n",
    "    sys.path.insert(0, REPO_DIR)\n",
    "\n",
    "# Configure ngrok\n",
    "conf.get_default().auth_token = NGROK_TOKEN\n",
    "\n",
    "# Import the FastAPI app directly\n",
    "from app.main import app\n",
    "\n",
    "# Run uvicorn in a background thread (same process = shared GPU memory)\n",
    "server_config = uvicorn.Config(\n",
    "    app, host='0.0.0.0', port=8000,\n",
    "    log_level='info', timeout_keep_alive=120\n",
    ")\n",
    "server = uvicorn.Server(server_config)\n",
    "\n",
    "server_thread = threading.Thread(target=server.run, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "print('Starting FastAPI server (in-process)...')\n",
    "print('Models load in background. First run downloads ~2GB + ~770MB.')\n",
    "\n",
    "# Wait for server to be ready\n",
    "server_ready = False\n",
    "for i in range(12):\n",
    "    try:\n",
    "        r = req.get('http://localhost:8000/api/health', timeout=3)\n",
    "        if r.status_code == 200:\n",
    "            server_ready = True\n",
    "            break\n",
    "    except:\n",
    "        pass\n",
    "    time.sleep(5)\n",
    "\n",
    "if server_ready:\n",
    "    print('Server is running on port 8000')\n",
    "else:\n",
    "    print('Server not responding yet, starting ngrok anyway...')\n",
    "\n",
    "# Open ngrok tunnel\n",
    "tunnel = ngrok.connect(8000)\n",
    "public_url = tunnel.public_url\n",
    "\n",
    "print()\n",
    "print('=' * 60)\n",
    "print(f'  SERVER IS LIVE!')\n",
    "print(f'  Public URL:     {public_url}')\n",
    "print(f'  Chat endpoint:  {public_url}/api/chat/hybrid')\n",
    "print(f'  Health check:   {public_url}/api/health')\n",
    "print('=' * 60)\n",
    "print()\n",
    "print('Copy the Public URL above and use it in your frontend!')\n",
    "print()\n",
    "print('Models are loading in background (~2-3 min).')\n",
    "print('Run Cell 6B to monitor, or wait and run Cell 7 to test.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 6B: Monitor download progress (run AFTER Cell 6)\n",
    "# ============================================================\n",
    "import os, time, requests as req\n",
    "\n",
    "HF_CACHE = os.path.expanduser('~/.cache/huggingface/hub')\n",
    "EXPECTED_MB = 2800  # ~2GB Phi-3 + ~770MB T5\n",
    "\n",
    "def get_folder_size_mb(path):\n",
    "    total = 0\n",
    "    if not os.path.exists(path):\n",
    "        return 0\n",
    "    for dirpath, _, filenames in os.walk(path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            try:\n",
    "                total += os.path.getsize(fp)\n",
    "            except OSError:\n",
    "                pass\n",
    "    return total / (1024 * 1024)\n",
    "\n",
    "def get_gpu_info():\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            used = torch.cuda.memory_allocated() / 1024**3\n",
    "            total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "            return f'{used:.1f}GB / {total:.1f}GB'\n",
    "    except Exception:\n",
    "        pass\n",
    "    return 'N/A'\n",
    "\n",
    "def check_model_status():\n",
    "    try:\n",
    "        r = req.get('http://localhost:8000/api/chat/hybrid/status', timeout=3)\n",
    "        return r.json()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "print('Monitoring download + loading progress...')\n",
    "print(f'Expected total: ~{EXPECTED_MB/1024:.1f} GB\\n')\n",
    "\n",
    "prev_size = 0\n",
    "for i in range(60):\n",
    "    size_mb = get_folder_size_mb(HF_CACHE)\n",
    "    pct = min(100, (size_mb / EXPECTED_MB) * 100)\n",
    "    speed = size_mb - prev_size\n",
    "    speed_per_sec = speed / 5 if speed > 0 else 0\n",
    "    gpu = get_gpu_info()\n",
    "    status = check_model_status()\n",
    "    \n",
    "    bar = chr(9608) * int(pct // 5) + chr(9617) * (20 - int(pct // 5))\n",
    "    print(f'  [{bar}] {pct:5.1f}% | {size_mb:.0f}/{EXPECTED_MB} MB | {speed_per_sec:.0f} MB/s | GPU: {gpu}')\n",
    "    \n",
    "    if status and status.get('phi3_loaded'):\n",
    "        print(f'\\nModels loaded! Pipeline: {status.get(\"pipeline\")}')\n",
    "        print('You can now run Cell 7 to test.')\n",
    "        break\n",
    "    \n",
    "    if status and not status.get('loading_in_progress') and status.get('load_attempts', 0) >= 3:\n",
    "        print(f'\\nLoading failed after {status.get(\"load_attempts\")} attempts.')\n",
    "        print('Try: Restart kernel, then re-run all cells.')\n",
    "        break\n",
    "    \n",
    "    prev_size = size_mb\n",
    "    time.sleep(5)\n",
    "else:\n",
    "    print('\\nStill loading after 5 min. Re-run this cell to keep monitoring.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 7: Test the AI with a sample query\n",
    "# ============================================================\n",
    "import requests as req\n",
    "import json\n",
    "\n",
    "try:\n",
    "    base_url = public_url\n",
    "except NameError:\n",
    "    base_url = 'http://localhost:8000'\n",
    "    print(f'ngrok URL not available, using {base_url}\\n')\n",
    "\n",
    "# Check model loading status\n",
    "print('Checking model status...')\n",
    "try:\n",
    "    status = req.get(f'{base_url}/api/chat/hybrid/status', timeout=10).json()\n",
    "    print(f'   Phi-3 loaded: {status.get(\"phi3_loaded\")}')\n",
    "    print(f'   Loading in progress: {status.get(\"loading_in_progress\")}')\n",
    "    print(f'   Load attempts: {status.get(\"load_attempts\")}/{status.get(\"max_attempts\")}')\n",
    "    print(f'   Pipeline: {status.get(\"pipeline\")}\\n')\n",
    "except Exception as e:\n",
    "    print(f'   Could not check status: {e}\\n')\n",
    "\n",
    "print('Sending test query: \"show me the francis gays expenses file\"')\n",
    "print('(First query may be slow ~30-60s while models warm up)\\n')\n",
    "\n",
    "try:\n",
    "    response = req.post(\n",
    "        f'{base_url}/api/chat/hybrid',\n",
    "        json={'query': 'show me the francis gays expenses file'},\n",
    "        timeout=180\n",
    "    )\n",
    "\n",
    "    print(f'Status: {response.status_code}')\n",
    "    data = response.json()\n",
    "    print(f'Pipeline: {data.get(\"metadata\", {}).get(\"pipeline\", \"unknown\")}')\n",
    "    print(f'SQL Source: {data.get(\"metadata\", {}).get(\"sql_source\", \"n/a\")}')\n",
    "    print(f'Message: {data.get(\"message\", \"\")}')\n",
    "    print(f'Results: {data.get(\"metadata\", {}).get(\"row_count\", 0)} rows')\n",
    "    \n",
    "    if data.get('metadata', {}).get('pipeline') == 'phi3+t5':\n",
    "        sql_src = data.get('metadata', {}).get('sql_source', '')\n",
    "        print(f'\\nFull AI pipeline is working! (Phi-3 + T5, SQL from: {sql_src})')\n",
    "    elif data.get('metadata', {}).get('pipeline') == 'rule-based':\n",
    "        print('\\nUsing rule-based fallback (Phi-3 may still be loading)')\n",
    "        print('Wait 1-2 minutes and try again.')\n",
    "    \n",
    "    print(f'\\nFull response:\\n{json.dumps(data, indent=2, ensure_ascii=False)}')\n",
    "\n",
    "except req.exceptions.Timeout:\n",
    "    print('Request timed out. Models may still be loading.')\n",
    "    print('Wait 2 minutes and re-run this cell.')\n",
    "except Exception as e:\n",
    "    print(f'Error: {e}')\n",
    "    print('Make sure Cell 6 completed successfully.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 7B: Test Query Gating (out-of-scope + vague queries)\n",
    "# ============================================================\n",
    "import requests as req\n",
    "import json\n",
    "\n",
    "try:\n",
    "    base_url = public_url\n",
    "except NameError:\n",
    "    base_url = 'http://localhost:8000'\n",
    "\n",
    "test_cases = [\n",
    "    {'label': 'Out-of-scope (weather)',   'query': 'what is the weather today?',       'expect_intent': 'out_of_scope'},\n",
    "    {'label': 'Out-of-scope (recipe)',    'query': 'how do I cook adobo?',             'expect_intent': 'out_of_scope'},\n",
    "    {'label': 'Vague (needs clarify)',    'query': 'show me data',                     'expect_intent': 'clarification'},\n",
    "    {'label': 'Valid query',              'query': 'show all expenses for SJDM project', 'expect_intent': None},\n",
    "]\n",
    "\n",
    "print('Testing Query Gating Features\\n')\n",
    "passed = 0\n",
    "for tc in test_cases:\n",
    "    print(f'--- {tc[\"label\"]} ---')\n",
    "    print(f'  Query: \"{tc[\"query\"]}\"')\n",
    "    try:\n",
    "        r = req.post(f'{base_url}/api/chat/hybrid', json={'query': tc['query']}, timeout=120)\n",
    "        data = r.json()\n",
    "        intent = data.get('intent', '')\n",
    "        msg = data.get('message', '')[:120]\n",
    "        pipeline = data.get('metadata', {}).get('pipeline', 'unknown')\n",
    "        print(f'  Intent: {intent} | Pipeline: {pipeline}')\n",
    "        print(f'  Message: {msg}')\n",
    "        if tc['expect_intent'] and intent == tc['expect_intent']:\n",
    "            print(f'  PASS (got expected intent: {tc[\"expect_intent\"]})')\n",
    "            passed += 1\n",
    "        elif tc['expect_intent'] is None and intent not in ('out_of_scope', 'error'):\n",
    "            print(f'  PASS (valid query processed normally)')\n",
    "            passed += 1\n",
    "        else:\n",
    "            print(f'  Expected intent={tc[\"expect_intent\"]}, got={intent}')\n",
    "    except Exception as e:\n",
    "        print(f'  Error: {e}')\n",
    "    print()\n",
    "\n",
    "print(f'Results: {passed}/{len(test_cases)} tests passed')\n",
    "if passed == len(test_cases):\n",
    "    print('Query gating is working correctly!')\n",
    "else:\n",
    "    print('Some tests did not match. Phi-3 is probabilistic, results may vary.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 8: Health check + Model status + Debug\n",
    "# ============================================================\n",
    "import requests as req\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    base_url = public_url\n",
    "except NameError:\n",
    "    base_url = 'http://localhost:8000'\n",
    "\n",
    "# Step 1: Check if server process is alive\n",
    "print('--- Step 1: Server process check ---')\n",
    "ps = subprocess.run(['pgrep', '-f', 'uvicorn'], capture_output=True, text=True)\n",
    "if ps.stdout.strip():\n",
    "    print(f'  Server PID(s): {ps.stdout.strip().replace(chr(10), \", \")}')\n",
    "else:\n",
    "    print('  SERVER IS DEAD - uvicorn process not found!')\n",
    "    print('  Restart kernel and re-run all cells.')\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            used = torch.cuda.memory_allocated() / 1024**3\n",
    "            total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "            print(f'  GPU VRAM: {used:.1f}GB / {total:.1f}GB')\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# Step 2: Health check\n",
    "print('\\n--- Step 2: Health endpoint ---')\n",
    "try:\n",
    "    r = req.get(f'{base_url}/api/health', timeout=10)\n",
    "    if r.status_code == 200:\n",
    "        print(f'  Health: {r.status_code} {r.json()}')\n",
    "    else:\n",
    "        print(f'  Health: {r.status_code} (raw: {r.text[:200]})')\n",
    "except req.exceptions.ConnectionError:\n",
    "    print('  Connection refused - server is not running.')\n",
    "except Exception as e:\n",
    "    print(f'  Error: {e}')\n",
    "\n",
    "# Step 3: Model status\n",
    "print('\\n--- Step 3: Model status ---')\n",
    "try:\n",
    "    r = req.get(f'{base_url}/api/chat/hybrid/status', timeout=10)\n",
    "    if r.status_code == 200:\n",
    "        status = r.json()\n",
    "        print(f'  Phi-3 loaded:       {status.get(\"phi3_loaded\")}')\n",
    "        print(f'  Loading in progress: {status.get(\"loading_in_progress\")}')\n",
    "        print(f'  Load attempts:      {status.get(\"load_attempts\")}/{status.get(\"max_attempts\")}')\n",
    "        print(f'  Active pipeline:    {status.get(\"pipeline\")}')\n",
    "        if status.get('phi3_loaded'):\n",
    "            print('\\n  READY! Run Cell 7 to test.')\n",
    "        elif status.get('loading_in_progress'):\n",
    "            print('\\n  Models still loading... wait and re-run this cell.')\n",
    "        else:\n",
    "            print(f'\\n  Models not loaded after {status.get(\"load_attempts\")} attempts.')\n",
    "            print('  Try: Restart kernel, then re-run all cells.')\n",
    "    else:\n",
    "        print(f'  Status: {r.status_code} (raw: {r.text[:200]})')\n",
    "except req.exceptions.ConnectionError:\n",
    "    print('  Connection refused - server is not running.')\n",
    "except Exception as e:\n",
    "    print(f'  Error: {e}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 9: Debug - Load Phi-3 directly (standalone, skip Cell 6)\n",
    "# ============================================================\n",
    "# WARNING: Do NOT run this if Cell 6 (server) is already running!\n",
    "#          Two Phi-3 instances will cause CUDA OOM.\n",
    "#          Use this ONLY for standalone debugging.\n",
    "# ============================================================\n",
    "import sys, os, traceback\n",
    "\n",
    "REPO_DIR = '/kaggle/working/auggregates-ai-data-look-up'\n",
    "os.chdir(REPO_DIR)\n",
    "if REPO_DIR not in sys.path:\n",
    "    sys.path.insert(0, REPO_DIR)\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "print('Attempting to load Phi-3+T5 directly...')\n",
    "print('(This will take 3-5 min on first run as it downloads ~2GB)\\n')\n",
    "\n",
    "try:\n",
    "    from app.services.phi3_service import Phi3Service\n",
    "    svc = Phi3Service()\n",
    "    print(f'Config: model={svc.config.model_name}')\n",
    "    print(f'Config: quant={svc.config.quantization}')\n",
    "    print(f'Config: device={svc.config.device}')\n",
    "    print()\n",
    "    svc._load_model()\n",
    "    print('\\n--- Model loaded! Testing inference ---')\n",
    "    import asyncio\n",
    "    result = asyncio.get_event_loop().run_until_complete(\n",
    "        svc.process_query('show all expenses', 'test-user')\n",
    "    )\n",
    "    print(f'Pipeline response: {result.get(\"response\", \"\")[:200]}')\n",
    "    print(f'SQL: {result.get(\"sql\", \"\")}')\n",
    "    print(f'Rows: {result.get(\"row_count\", 0)}')\n",
    "    print('\\nPhi-3+T5 is working!')\n",
    "except Exception as e:\n",
    "    print(f'Error: {type(e).__name__}: {e}')\n",
    "    traceback.print_exc()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Notes\n",
    "\n",
    "**Kaggle vs Colab differences:**\n",
    "- Kaggle gives 30 hours/week of GPU (vs Colab's ~90 min free sessions)\n",
    "- Sessions can run up to 12 hours continuously\n",
    "- Secrets are in Settings > Add-ons > Secrets (not sidebar key icon)\n",
    "- Internet must be explicitly enabled in Settings\n",
    "\n",
    "**Keeping the server alive:**\n",
    "- Kaggle sessions stay alive longer than Colab\n",
    "- Keep the browser tab open to prevent disconnection\n",
    "- If disconnected, re-run all cells from Cell 1\n",
    "\n",
    "**Connecting your frontend:**\n",
    "- Use the ngrok Public URL from Cell 6 as your API base URL\n",
    "- The URL changes every restart\n",
    "\n",
    "**Performance:**\n",
    "- First query after startup: ~30-60 seconds (model warmup)\n",
    "- Subsequent queries: ~5-15 seconds\n",
    "- T4 GPU has 15GB VRAM, Phi-3 4-bit uses ~2GB\n",
    "\n",
    "**Troubleshooting:**\n",
    "- `CUDA out of memory` > Restart kernel, re-run all cells\n",
    "- `ngrok error` > Check your auth token\n",
    "- `rule-based fallback` > Models still loading, wait 1-2 min"
   ]
  }
 ]
}