{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "kaggle": {
   "accelerator": "gpu",
   "isGpuEnabled": true
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83c\udfaf T5 Fine-Tuning \u2014 AU-Ggregates Text-to-SQL (Kaggle)\n",
    "\n",
    "Fine-tunes `gaussalgo/T5-LM-Large-text2sql-spider` on your custom + Spider training data.\n",
    "\n",
    "## Before you start\n",
    "1. Go to **Settings** (right panel) \u2192 **Accelerator** \u2192 select **GPU T4 x2** or **P100**\n",
    "2. Click **Add Data** (right panel) \u2192 upload your `t5_text2sql_5000_pairs.jsonl`\n",
    "3. Your uploaded file will be at `/kaggle/input/YOUR_DATASET_NAME/t5_text2sql_5000_pairs.jsonl`\n",
    "\n",
    "## Pipeline\n",
    "| Step | Cell | What it does | Time |\n",
    "|------|------|-------------|------|\n",
    "| 1 | Install | Install dependencies | ~2 min |\n",
    "| 2 | GPU Check | Verify GPU | instant |\n",
    "| 3 | Load Data | Auto-find uploaded JSONL file | instant |\n",
    "| 4 | Validate | Validate all 5,000 pairs | ~10 sec |\n",
    "| 5 | Clean + Merge | Clean custom pairs + merge Spider data | ~3-5 min |\n",
    "| 6 | Train | Fine-tune T5 (10 epochs) | ~60-120 min |\n",
    "| 7 | Evaluate | Test accuracy on validation set | ~5 min |\n",
    "| 8 | Test | Interactive SQL generation test | instant |\n",
    "| 9 | Export | Zip model for download | ~2 min |"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 1: Install dependencies\n",
    "# ============================================================\n",
    "# CRITICAL: Must set BEFORE any torch import to prevent DataParallel NaN\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "!pip install -q transformers accelerate sentencepiece\n",
    "!pip install -q datasets evaluate sqlparse\n",
    "!pip install -q huggingface_hub peft\n",
    "\n",
    "print('\\nAll dependencies installed!')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 2: Verify GPU\n",
    "# ==================================sa==========================\n",
    "import torch\n",
    "\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'CUDA:    {torch.cuda.is_available()}')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    print(f'GPUs:    {n_gpus}')\n",
    "    for i in range(n_gpus):\n",
    "        name = torch.cuda.get_device_name(i)\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        vram = (getattr(props, 'total_memory', None) or getattr(props, 'total_mem', 0)) / 1024**3\n",
    "        print(f'  GPU {i}: {name} ({vram:.1f} GB VRAM)')\n",
    "    print('\\n\u2705 GPU ready!')\n",
    "else:\n",
    "    print('\u274c No GPU! Go to Settings \u2192 Accelerator \u2192 GPU T4 x2')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 3: Load training data (auto-detect from /kaggle/input/)\n",
    "# ============================================================\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Auto-find the JSONL file in /kaggle/input/\n",
    "matches = glob.glob('/kaggle/input/**/*.jsonl', recursive=True)\n",
    "\n",
    "if matches:\n",
    "    TRAINING_FILE = matches[0]\n",
    "    print(f'\ud83d\udcc1 Found: {TRAINING_FILE}')\n",
    "    print(f'   Size: {os.path.getsize(TRAINING_FILE) / 1024 / 1024:.1f} MB')\n",
    "else:\n",
    "    print('\u274c No JSONL file found in /kaggle/input/')\n",
    "    print('   Click \"Add Data\" in the right panel and upload your .jsonl file')\n",
    "    print()\n",
    "    print('   Available files in /kaggle/input/:')\n",
    "    for f in glob.glob('/kaggle/input/**/*', recursive=True):\n",
    "        print(f'     {f}')\n",
    "    TRAINING_FILE = None\n",
    "\n",
    "print(f'\\n\u2705 TRAINING_FILE = {TRAINING_FILE}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 4: Validate training data\n",
    "# ============================================================\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "SCHEMA_PREFIX = 'tables: ai_documents (id, source_table, file_name, project_name, searchable_text, metadata, document_type) | query:'\n",
    "VALID_SOURCE_TABLES = {'Expenses', 'CashFlow', 'Project', 'Quotation', 'QuotationItem'}\n",
    "NUMERIC_KEYS = {'Expenses', 'Amount', 'total_amount', 'volume', 'line_total'}\n",
    "\n",
    "pairs = []\n",
    "errors = []\n",
    "source_table_counts = Counter()\n",
    "intent_counts = Counter()\n",
    "\n",
    "with open(TRAINING_FILE, 'r', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f, 1):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            pair = json.loads(line)\n",
    "        except json.JSONDecodeError as e:\n",
    "            errors.append(f'Line {i}: Invalid JSON - {e}')\n",
    "            continue\n",
    "\n",
    "        inp = pair.get('input', '')\n",
    "        tgt = pair.get('target', '')\n",
    "\n",
    "        if 'input' not in pair or 'target' not in pair:\n",
    "            errors.append(f'Line {i}: Missing input or target key')\n",
    "            continue\n",
    "\n",
    "        if not inp.startswith(SCHEMA_PREFIX):\n",
    "            errors.append(f'Line {i}: Missing Spider schema prefix')\n",
    "\n",
    "        tgt_upper = tgt.strip().upper()\n",
    "        if not tgt_upper.startswith('SELECT'):\n",
    "            errors.append(f'Line {i}: Target is not a SELECT statement')\n",
    "\n",
    "        has_source = any(f\"source_table = '{t}'\" in tgt for t in VALID_SOURCE_TABLES)\n",
    "        if not has_source:\n",
    "            errors.append(f'Line {i}: Missing source_table filter')\n",
    "\n",
    "        if \"document_type = 'file'\" not in tgt and \"document_type = 'row'\" not in tgt:\n",
    "            errors.append(f'Line {i}: Missing document_type filter')\n",
    "\n",
    "        for t in VALID_SOURCE_TABLES:\n",
    "            if f\"source_table = '{t}'\" in tgt:\n",
    "                source_table_counts[t] += 1\n",
    "\n",
    "        if 'SUM(' in tgt_upper:\n",
    "            intent_counts['sum'] += 1\n",
    "        elif 'AVG(' in tgt_upper:\n",
    "            intent_counts['average'] += 1\n",
    "        elif 'COUNT(' in tgt_upper:\n",
    "            intent_counts['count'] += 1\n",
    "        elif 'GROUP BY' in tgt_upper and ('SUM' in tgt_upper or 'COUNT' in tgt_upper):\n",
    "            intent_counts['compare'] += 1\n",
    "        elif 'DISTINCT' in tgt_upper:\n",
    "            intent_counts['list_categories'] += 1\n",
    "        elif \"document_type = 'file'\" in tgt:\n",
    "            intent_counts['list_files'] += 1\n",
    "        else:\n",
    "            intent_counts['query_data'] += 1\n",
    "\n",
    "        pairs.append(pair)\n",
    "\n",
    "print('=' * 60)\n",
    "print(f'\ud83d\udcca VALIDATION REPORT')\n",
    "print('=' * 60)\n",
    "print(f'Total pairs:  {len(pairs)}')\n",
    "print(f'Errors:       {len(errors)}')\n",
    "print()\n",
    "print('Source table distribution:')\n",
    "for t in sorted(source_table_counts, key=source_table_counts.get, reverse=True):\n",
    "    pct = source_table_counts[t] / len(pairs) * 100\n",
    "    print(f'  {t:20s} {source_table_counts[t]:5d} ({pct:.1f}%)')\n",
    "print()\n",
    "print('Intent distribution:')\n",
    "for intent in sorted(intent_counts, key=intent_counts.get, reverse=True):\n",
    "    pct = intent_counts[intent] / len(pairs) * 100\n",
    "    print(f'  {intent:20s} {intent_counts[intent]:5d} ({pct:.1f}%)')\n",
    "print()\n",
    "\n",
    "if errors:\n",
    "    print(f'\u26a0\ufe0f  First 10 errors:')\n",
    "    for e in errors[:10]:\n",
    "        print(f'  {e}')\n",
    "    print()\n",
    "\n",
    "if len(pairs) >= 4500 and len(errors) < len(pairs) * 0.05:\n",
    "    print(f'\u2705 Data looks good! {len(pairs)} valid pairs ready.')\n",
    "elif len(pairs) > 0:\n",
    "    print(f'\u26a0\ufe0f  {len(pairs)} valid pairs. Error rate: {len(errors)/max(len(pairs),1)*100:.1f}%')\n",
    "else:\n",
    "    print('\u274c No valid pairs found! Check your JSONL file.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 5: Clean custom pairs + Merge with Spider dataset\n",
    "# ============================================================\n",
    "import json, re, hashlib, random\n",
    "from collections import Counter\n",
    "from datasets import load_dataset\n",
    "\n",
    "SPIDER_LIMIT = 3000\n",
    "\n",
    "def normalize_sql(sql):\n",
    "    sql = sql.strip().rstrip(';').strip()\n",
    "    sql = re.sub(r'\\\\s+', ' ', sql)\n",
    "    return sql.lower()\n",
    "\n",
    "def sql_fingerprint(pair):\n",
    "    return hashlib.md5(normalize_sql(pair.get('target', '')).encode()).hexdigest()\n",
    "\n",
    "TYPO_FIXES = {\n",
    "    \"source_table = 'expenses'\": \"source_table = 'Expenses'\",\n",
    "    \"source_table = 'cashflow'\": \"source_table = 'CashFlow'\",\n",
    "    \"source_table = 'Cashflow'\": \"source_table = 'CashFlow'\",\n",
    "    \"source_table = 'cash_flow'\": \"source_table = 'CashFlow'\",\n",
    "    \"source_table = 'project'\": \"source_table = 'Project'\",\n",
    "    \"source_table = 'quotation'\": \"source_table = 'Quotation'\",\n",
    "    \"source_table = 'quotationitem'\": \"source_table = 'QuotationItem'\",\n",
    "    \"source_table = 'QuotationItems'\": \"source_table = 'QuotationItem'\",\n",
    "    \"source_table = 'Quotation_Item'\": \"source_table = 'QuotationItem'\",\n",
    "}\n",
    "\n",
    "def clean_pair(pair):\n",
    "    tgt = pair['target'].strip()\n",
    "    if not tgt.endswith(';'):\n",
    "        tgt += ';'\n",
    "    tgt = tgt.replace(';;', ';')\n",
    "    for wrong, right in TYPO_FIXES.items():\n",
    "        tgt = tgt.replace(wrong, right)\n",
    "    tgt = re.sub(r\"metadata->'(\\\\w+)'\", r\"metadata->>'\\\\1'\", tgt)\n",
    "    pair['target'] = tgt\n",
    "    return pair\n",
    "\n",
    "def validate_custom(pair):\n",
    "    inp = pair.get('input', '')\n",
    "    tgt = pair.get('target', '')\n",
    "    if not inp.startswith(SCHEMA_PREFIX):\n",
    "        return False\n",
    "    if not tgt.strip().upper().startswith('SELECT'):\n",
    "        return False\n",
    "    has_src = any(f\"source_table = '{t}'\" in tgt for t in VALID_SOURCE_TABLES)\n",
    "    if not has_src:\n",
    "        return False\n",
    "    if \"document_type = 'file'\" not in tgt and \"document_type = 'row'\" not in tgt:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# --- Step 1: Clean custom pairs ---\n",
    "print('\ud83e\uddf9 Step 1: Cleaning custom pairs...')\n",
    "cleaned = [clean_pair(dict(p)) for p in pairs]\n",
    "valid_custom = [p for p in cleaned if validate_custom(p)]\n",
    "n_invalid = len(cleaned) - len(valid_custom)\n",
    "print(f'   Valid: {len(valid_custom)}, Invalid: {n_invalid}')\n",
    "\n",
    "# --- Step 2: Download Spider dataset ---\n",
    "print(f'\\n\ud83d\udd77\ufe0f Step 2: Downloading Spider dataset (limit={SPIDER_LIMIT})...')\n",
    "spider_ds = load_dataset('spider', split='train')\n",
    "spider_pairs = []\n",
    "spider_seen = set()\n",
    "\n",
    "for ex in spider_ds:\n",
    "    q = ex.get('question', '')\n",
    "    sql = ex.get('query', '')\n",
    "    db = ex.get('db_id', '')\n",
    "    if not q or not sql:\n",
    "        continue\n",
    "    if not sql.strip().upper().startswith('SELECT'):\n",
    "        continue\n",
    "    tgt = sql.strip()\n",
    "    if not tgt.endswith(';'):\n",
    "        tgt += ';'\n",
    "    sfp = hashlib.md5(normalize_sql(tgt).encode()).hexdigest()\n",
    "    if sfp in spider_seen:\n",
    "        continue\n",
    "    spider_seen.add(sfp)\n",
    "    spider_pairs.append({\n",
    "        'input': f'tables: {db} | query: {q}',\n",
    "        'target': tgt\n",
    "    })\n",
    "    if len(spider_pairs) >= SPIDER_LIMIT:\n",
    "        break\n",
    "\n",
    "print(f'   Spider pairs loaded: {len(spider_pairs)}')\n",
    "\n",
    "# --- Step 3: Merge + Deduplicate ---\n",
    "print('\\n\ud83d\udd00 Step 3: Merging + deduplicating...')\n",
    "all_merged = valid_custom + spider_pairs\n",
    "seen_fps = set()\n",
    "deduped = []\n",
    "n_dupes = 0\n",
    "\n",
    "for p in all_merged:\n",
    "    fp = sql_fingerprint(p)\n",
    "    if fp not in seen_fps:\n",
    "        seen_fps.add(fp)\n",
    "        deduped.append(p)\n",
    "    else:\n",
    "        n_dupes += 1\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(deduped)\n",
    "\n",
    "# --- Step 4: Save merged file ---\n",
    "MERGED_FILE = '/kaggle/working/training_final.jsonl'\n",
    "with open(MERGED_FILE, 'w', encoding='utf-8') as f:\n",
    "    for p in deduped:\n",
    "        f.write(json.dumps(p, ensure_ascii=False) + '\\n')\n",
    "\n",
    "TRAINING_FILE = MERGED_FILE\n",
    "\n",
    "print()\n",
    "print('=' * 60)\n",
    "print('  CLEAN + MERGE REPORT')\n",
    "print('=' * 60)\n",
    "print(f'  Custom pairs (valid):    {len(valid_custom)}')\n",
    "print(f'  Custom pairs (invalid):  {n_invalid}')\n",
    "print(f'  Spider pairs:            {len(spider_pairs)}')\n",
    "print(f'  Duplicates removed:      {n_dupes}')\n",
    "print(f'  Total merged:            {len(deduped)}')\n",
    "print(f'  Output file:             {MERGED_FILE}')\n",
    "print('=' * 60)\n",
    "print()\n",
    "if len(deduped) >= 5000:\n",
    "    print(f'\u2705 {len(deduped)} pairs ready for training!')\n",
    "else:\n",
    "    print(f'\u26a0\ufe0f  Only {len(deduped)} pairs. Consider adding more custom data.')\n",
    "print(f'\\n\ud83d\udccc TRAINING_FILE updated to: {TRAINING_FILE}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 6: Fine-tune T5 with LoRA (PEFT) -- bulletproof version\n",
    "# ============================================================\n",
    "# LoRA = Low-Rank Adaptation. Freezes base model, trains ~2.4M\n",
    "# adapter params (~0.3%). After training, merges back to normal T5.\n",
    "# ============================================================\n",
    "import json, time, gc, torch\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM, AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# --- Verify single GPU ---\n",
    "assert torch.cuda.device_count() == 1, (\n",
    "    f'Expected 1 GPU but found {torch.cuda.device_count()}! '\n",
    "    'Restart kernel and make sure Cell 1 has os.environ[CUDA_VISIBLE_DEVICES]=0'\n",
    ")\n",
    "print(f'GPU: {torch.cuda.get_device_name(0)} (single GPU mode)')\n",
    "\n",
    "# --- Config ---\n",
    "MODEL_NAME = 'gaussalgo/T5-LM-Large-text2sql-spider'\n",
    "OUTPUT_DIR = '/kaggle/working/t5-finetuned'\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 2\n",
    "LEARNING_RATE = 1e-4     # safer than 3e-4 for fp16 LoRA\n",
    "MAX_INPUT_LEN = 512\n",
    "MAX_TARGET_LEN = 256\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.05\n",
    "\n",
    "# --- Load data ---\n",
    "print('Loading training data...')\n",
    "inputs, targets = [], []\n",
    "with open(TRAINING_FILE, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            pair = json.loads(line)\n",
    "            if 'input' in pair and 'target' in pair:\n",
    "                inputs.append(pair['input'])\n",
    "                targets.append(pair['target'])\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "dataset = Dataset.from_dict({'input': inputs, 'target': targets})\n",
    "split = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_ds, val_ds = split['train'], split['test']\n",
    "print(f'   Train: {len(train_ds)}, Validation: {len(val_ds)}')\n",
    "\n",
    "# --- Load model in float32 first, then LoRA ---\n",
    "print(f'Loading {MODEL_NAME}...')\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "print(f'   Base parameters: {base_model.num_parameters():,}')\n",
    "\n",
    "base_model.gradient_checkpointing_enable()\n",
    "print('   Gradient checkpointing: ON')\n",
    "\n",
    "print('Applying LoRA adapter...')\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    r=LORA_R, lora_alpha=LORA_ALPHA, lora_dropout=LORA_DROPOUT,\n",
    "    target_modules=['q', 'v'],\n",
    ")\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# --- Tokenize ---\n",
    "# NOTE: Do NOT use padding='max_length' here -- let DataCollatorForSeq2Seq handle padding.\n",
    "# Only truncate. The collator will pad dynamically per batch and apply -100 masking.\n",
    "def tokenize(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples['input'], max_length=MAX_INPUT_LEN,\n",
    "        truncation=True\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        text_target=examples['target'], max_length=MAX_TARGET_LEN,\n",
    "        truncation=True\n",
    "    )\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "print('Tokenizing...')\n",
    "tok_train = train_ds.map(tokenize, batched=True, remove_columns=['input', 'target'])\n",
    "tok_val = val_ds.map(tokenize, batched=True, remove_columns=['input', 'target'])\n",
    "\n",
    "# --- DIAGNOSTIC: Verify labels are NOT empty ---\n",
    "print('\\n\ud83d\udd0d Label diagnostic (first 3 examples):')\n",
    "for i in range(min(3, len(tok_train))):\n",
    "    lbl = tok_train[i]['labels']\n",
    "    print(f'  Example {i}: {len(lbl)} label tokens, first 5: {lbl[:5]}')\n",
    "    if len(lbl) == 0:\n",
    "        raise ValueError(f'Example {i} has EMPTY labels! Check training data.')\n",
    "print()\n",
    "\n",
    "# --- Training args ---\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_loss',\n",
    "    greater_is_better=False,\n",
    "    save_total_limit=2,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    report_to='none',\n",
    "    dataloader_num_workers=0,\n",
    "    warmup_steps=200,\n",
    "    weight_decay=0.01,\n",
    "    max_grad_norm=1.0,\n",
    "    gradient_accumulation_steps=4,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tok_train,\n",
    "    eval_dataset=tok_val,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, padding=True),\n",
    ")\n",
    "\n",
    "# --- Train ---\n",
    "gc.collect(); torch.cuda.empty_cache()\n",
    "steps_per_epoch = len(tok_train) // (BATCH_SIZE * 4)\n",
    "print(f'\\nStarting LoRA training: {EPOCHS} epochs, batch={BATCH_SIZE}, lr={LEARNING_RATE}')\n",
    "print(f'   Steps per epoch: ~{steps_per_epoch}')\n",
    "print(f'   Estimated time: ~{EPOCHS * steps_per_epoch * 0.5 / 60:.0f} minutes on single T4')\n",
    "print(f'   Single GPU: {torch.cuda.get_device_name(0)}\\n')\n",
    "\n",
    "start = time.time()\n",
    "trainer.train()\n",
    "elapsed = (time.time() - start) / 60\n",
    "\n",
    "# --- Merge LoRA adapter back into base model ---\n",
    "print('\\nMerging LoRA adapter into base model...')\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "# --- Save merged model ---\n",
    "save_path = f'{OUTPUT_DIR}/final'\n",
    "merged_model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "model = merged_model\n",
    "\n",
    "print(f'\\nTraining complete in {elapsed:.1f} minutes!')\n",
    "print(f'   Model saved to: {save_path}')\n",
    "print(f'   LoRA adapter merged -- this is a normal T5 model now.')\n",
    "\n",
    "# --- AUTO-PUSH TO HUGGINGFACE (runs automatically after training) ---\n",
    "print('\\n' + '='*60)\n",
    "print('PUSHING TO HUGGINGFACE HUB...')\n",
    "print('='*60)\n",
    "try:\n",
    "    from huggingface_hub import login, HfApi\n",
    "    HF_TOKEN = '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'\n",
    "    HF_REPO = 't5-auggregates-text2sql'\n",
    "    login(token=HF_TOKEN)\n",
    "    api = HfApi()\n",
    "    whoami = api.whoami()\n",
    "    hf_username = whoami['name']\n",
    "    full_repo = f'{hf_username}/{HF_REPO}'\n",
    "    print(f'   Logged in as: {hf_username}')\n",
    "    print(f'   Repo: {full_repo}')\n",
    "    api.create_repo(HF_REPO, exist_ok=True, private=False)\n",
    "    print('   Uploading model files...')\n",
    "    merged_model.push_to_hub(full_repo)\n",
    "    tokenizer.push_to_hub(full_repo)\n",
    "    print(f'\\n✅ Model pushed! https://huggingface.co/{full_repo}')\n",
    "    print(f'   Set in .env: T5_MODEL_PATH={full_repo}')\n",
    "except Exception as e:\n",
    "    print(f'\\n❌ HuggingFace push failed: {e}')\n",
    "    print('   Model is still saved locally at:', save_path)\n",
    "    print('   Run Cell 9B manually to retry the push.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 7: Evaluate on validation set\n",
    "# ============================================================\n",
    "import sqlparse\n",
    "import time\n",
    "\n",
    "print('\ud83d\udcca Evaluating on validation set...\\n')\n",
    "\n",
    "model.eval()\n",
    "device = model.device\n",
    "\n",
    "exact_matches = 0\n",
    "valid_sql = 0\n",
    "total_time = 0.0\n",
    "samples = []\n",
    "\n",
    "for idx in range(len(val_ds)):\n",
    "    inp = val_ds[idx]['input']\n",
    "    expected = val_ds[idx]['target']\n",
    "\n",
    "    encoded = tokenizer(inp, return_tensors='pt', max_length=MAX_INPUT_LEN, truncation=True, padding=True)\n",
    "    input_ids = encoded.input_ids.to(device)\n",
    "    attention_mask = encoded.attention_mask.to(device)\n",
    "\n",
    "    t0 = time.time()\n",
    "    outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=MAX_TARGET_LEN)\n",
    "    total_time += (time.time() - t0) * 1000\n",
    "\n",
    "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "    if generated == expected:\n",
    "        exact_matches += 1\n",
    "\n",
    "    try:\n",
    "        parsed = sqlparse.parse(generated)\n",
    "        if parsed and parsed[0].get_type() and parsed[0].get_type().upper() == 'SELECT':\n",
    "            valid_sql += 1\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    if len(samples) < 15:\n",
    "        match = '\u2705' if generated == expected else '\u274c'\n",
    "        samples.append((match, inp.split('query: ')[-1], expected, generated))\n",
    "\n",
    "    if (idx + 1) % 100 == 0:\n",
    "        print(f'  Evaluated {idx + 1}/{len(val_ds)}...')\n",
    "\n",
    "total = len(val_ds)\n",
    "em_acc = exact_matches / total * 100\n",
    "exec_acc = valid_sql / total * 100\n",
    "avg_ms = total_time / total\n",
    "\n",
    "print()\n",
    "print('=' * 60)\n",
    "print('  EVALUATION RESULTS')\n",
    "print('=' * 60)\n",
    "print(f'  Validation examples:   {total}')\n",
    "print(f'  Exact-match accuracy:  {em_acc:.1f}%')\n",
    "print(f'  Valid SQL rate:        {exec_acc:.1f}%')\n",
    "print(f'  Avg inference time:    {avg_ms:.1f} ms')\n",
    "print('=' * 60)\n",
    "print()\n",
    "\n",
    "if em_acc >= 70:\n",
    "    print('\ud83c\udf89 Great accuracy! Model is ready for deployment.')\n",
    "elif em_acc >= 50:\n",
    "    print('\ud83d\udc4d Decent accuracy. Consider more training data or epochs.')\n",
    "else:\n",
    "    print('\u26a0\ufe0f  Low accuracy. Check training data quality or try more epochs.')\n",
    "\n",
    "print()\n",
    "print('Sample predictions:')\n",
    "print('-' * 60)\n",
    "for match, question, expected, generated in samples[:10]:\n",
    "    print(f'{match} Q: {question}')\n",
    "    print(f'   Expected:  {expected[:100]}')\n",
    "    print(f'   Generated: {generated[:100]}')\n",
    "    print()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 8: Interactive test \u2014 try your own questions\n",
    "# ============================================================\n",
    "\n",
    "SCHEMA_PREFIX_Q = 'tables: ai_documents (id, source_table, file_name, project_name, searchable_text, metadata, document_type) | query: '\n",
    "\n",
    "def generate_sql(question):\n",
    "    full_input = SCHEMA_PREFIX_Q + question\n",
    "    encoded = tokenizer(full_input, return_tensors='pt', max_length=MAX_INPUT_LEN, truncation=True, padding=True)\n",
    "    input_ids = encoded.input_ids.to(model.device)\n",
    "    attention_mask = encoded.attention_mask.to(model.device)\n",
    "    outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=MAX_TARGET_LEN)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "test_questions = [\n",
    "    'show all expense files',\n",
    "    'what are the total fuel expenses',\n",
    "    'how many labor entries are in project alpha',\n",
    "    'total cash flow amount for highway 5',\n",
    "    'show approved quotations for manila tower',\n",
    "    'total volume delivered for plate ABC-1234',\n",
    "    'list all active projects',\n",
    "    'average expense amount for materials',\n",
    "    'how many deliveries used 10-wheeler trucks',\n",
    "    'compare fuel costs between manila tower and building c',\n",
    "]\n",
    "\n",
    "print('\ud83e\uddea Testing SQL generation:\\n')\n",
    "for q in test_questions:\n",
    "    sql = generate_sql(q)\n",
    "    print(f'Q: {q}')\n",
    "    print(f'\u2192 {sql}')\n",
    "    print()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 8B: Try your own question (edit and re-run)\n",
    "# ============================================================\n",
    "\n",
    "my_question = 'total expenses for SJDM project'  # <-- edit this!\n",
    "\n",
    "sql = generate_sql(my_question)\n",
    "print(f'Q: {my_question}')\n",
    "print(f'\u2192 {sql}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 9: Export model (zip for download)\n",
    "# ============================================================\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "FINAL_MODEL = '/kaggle/working/t5-finetuned/final'\n",
    "\n",
    "print('\ud83d\udce6 Zipping model for download...')\n",
    "zip_path = shutil.make_archive('/kaggle/working/t5-finetuned-model', 'zip', FINAL_MODEL)\n",
    "print(f'   Size: {Path(zip_path).stat().st_size / 1024 / 1024:.0f} MB')\n",
    "\n",
    "print(f'\\n\u2705 Model zipped at: {zip_path}')\n",
    "print('\\n\ud83d\udce5 To download:')\n",
    "print('   1. Click the \"Output\" tab in the right panel')\n",
    "print('   2. Find t5-finetuned-model.zip')\n",
    "print('   3. Click the download icon')\n",
    "print()\n",
    "print('   Or save this notebook as a Kaggle Dataset to reuse the model.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CELL 9B: Push to HuggingFace Hub (optional)\n",
    "# ============================================================\n",
    "# Uncomment and fill in your details to push to HF Hub\n",
    "\n",
    "# from huggingface_hub import login\n",
    "# login(token='YOUR_HF_TOKEN')\n",
    "#\n",
    "# HF_REPO = 'your-username/t5-auggregates-text2sql'  # <-- change this!\n",
    "#\n",
    "# model.push_to_hub(HF_REPO)\n",
    "# tokenizer.push_to_hub(HF_REPO)\n",
    "# print(f'\u2705 Pushed to https://huggingface.co/{HF_REPO}')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## \ud83d\udcdd After Training\n",
    "\n",
    "**To use the fine-tuned model in production:**\n",
    "\n",
    "1. Download the zip from Cell 9 (Output tab) or use the HF Hub link from Cell 9B\n",
    "2. Extract to a folder on your server\n",
    "3. Set `T5_MODEL_PATH` environment variable to the extracted folder path\n",
    "4. The AI server will automatically use your fine-tuned model\n",
    "\n",
    "**Expected accuracy targets:**\n",
    "- Exact-match: 70%+ (good), 80%+ (great)\n",
    "- Valid SQL rate: 95%+\n",
    "\n",
    "**If accuracy is low:**\n",
    "- Check training data quality (run Cell 4 validation)\n",
    "- Try more epochs (change EPOCHS in Cell 6)\n",
    "- Try lower learning rate (1e-4 instead of 3e-4)\n",
    "- Add more diverse training pairs\n",
    "\n",
    "**Kaggle tips:**\n",
    "- GPU sessions last up to 12 hours\n",
    "- You get 30 hours/week of GPU time\n",
    "- Save your notebook to keep the output files"
   ]
  }
 ]
}